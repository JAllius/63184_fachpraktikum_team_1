from mlcore.io.preset_loader import loader
from mlcore.io.data_reader import get_dataframe_from_csv, preprocess_dataframe, get_semantic_types
from mlcore.io.model_saver import save_model
from mlcore.io.metadata_saver import save_metadata
from mlcore.profile.profiler import suggest_profile
from mlcore.explain.explanator import explain_model
from mlcore.metrics.metrics_calculator import calculate_metrics
from mlcore.metrics.cv_calculator import calculate_cv
from mlcore.explain.get_feature_names import get_feature_names
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from typing import Literal, Tuple
import pandas as pd
from db.db import db_get_dataset_version, get_ml_problem, create_model, update_model
import json
from pathlib import Path
import logging
logger = logging.getLogger(__name__)

BASE_DIR = "./testdata/models"
PRESET_DIR = "/code/mlcore/presets"
NAME = None

def train(
    name: str,
    problem_id: str,
    model_id: str | None = None,
    model_uri: str | None = None,
    algorithm: str = "auto",
    train_mode: Literal["fast", "balanced", "accurate"] = "balanced",
    evaluation_strategy: Literal["cv", "holdout"] = "cv",
    explain: bool = True,
    test_size_ratio: float = 0.2,
    random_seed: int = 42,
    preset_dir: str = PRESET_DIR,
) -> Tuple[str, str]:

    problem = get_ml_problem(problem_id)
    dataset_version_id = problem.get("dataset_version_id", False)
    target = problem.get("target", False)
    dataset_version = db_get_dataset_version(dataset_version_id)

    df = get_dataframe_from_csv(
        dataset_version.get("uri", False))
    raw_profile = dataset_version.get("profile_json")
    profile = json.loads(raw_profile) if isinstance(
        raw_profile, str) else raw_profile
    if not profile:
        profile = suggest_profile(pd.DataFrame(df))
    # multi_class = profile.get("columns", {}).get(
    #     target, {}).get("cardinality", 0) > 2

    X, y = preprocess_dataframe(df, target, profile)
    semantic_types = get_semantic_types(X, profile)
    task = problem.get("task")

    categorical = semantic_types["categorical"]
    numeric = semantic_types["numeric"]
    boolean = semantic_types["boolean"]

    if task == "classification":
        X_train, X_test, y_train, y_test = train_test_split(
            # stratify to keep class proportions
            X, y, test_size=test_size_ratio, stratify=y, random_state=random_seed
        )
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            # no stratify for regression
            X, y, test_size=test_size_ratio, random_state=random_seed
        )

    label_encoder = None
    if task == "classification":
        label_encoder = LabelEncoder()
        label_encoder.fit(y_train)
        y_train = label_encoder.transform(y_train)
        y_test = label_encoder.transform(y_test)

    build_model = loader(task, algorithm.lower(), preset_dir)

    model, metadata = build_model(categorical, numeric, boolean, train_mode)

    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    if label_encoder is not None:
        y_test_dec = label_encoder.inverse_transform(y_test)
        y_pred_dec = label_encoder.inverse_transform(y_pred)
    else:
        y_test_dec = y_test
        y_pred_dec = y_pred

    metrics = calculate_metrics(y_test_dec, y_pred_dec, task) #, multi_class)
    if evaluation_strategy == "cv":
        cv = calculate_cv(model, X_train, y_train, task) #, multi_class)
        metadata["cross_validation"] = cv

    explaination_summary = {}
    label_classes = None
    if task == "classification":
        label_classes = label_encoder.classes_.tolist()

    if explain:
        model_shap = model.named_steps.get("est")
        preprocessor = model.named_steps.get("pre")

        # Get feature names from transformed output
        feature_info = get_feature_names(preprocessor)
        feature_names = feature_info["feature_names"]
        feature_parents = feature_info["feature_parents"]

        # Store in metadata for UI + future use
        metadata["feature_names"] = feature_names
        metadata["feature_parents"] = feature_parents

        X_train_shap = preprocessor.transform(X_train)
        X_test_shap = preprocessor.transform(X_test)
        # explanation = explain_model(task, model_shap, X_train_shap, X_test_shap)
        explaination_summary = explain_model(
            task=task,
            model=model_shap,
            X_train=X_train_shap,
            X_test=X_test_shap,
            feature_names=feature_names,
            feature_parents=feature_parents,
            label_classes=label_classes,
            n_ref_max=200,
            n_explain_max=500,
            top_k=30,
            include_distributions=True,
            random_seed=random_seed,
        )

    metadata["model_name"] = name
    metadata["problem_id"] = problem_id
    metadata["target"] = target
    metadata["schema_snapshot"]["X"] = {
        column: str(X[column].dtype) for column in X.columns
    }
    metadata["schema_snapshot"]["y"] = {
        y.name: str(y.dtype)
    }
    metadata["schema_snapshot"]["feature_order"] = list(X.columns)
    metadata["metrics"] = metrics
    if explaination_summary:
        metadata["explanation"] = explaination_summary
    
    if task == "classification":
        metadata["label_classes"] = label_classes

    if not model_id and not model_uri:
        model_id, model_uri = create_model(
            problem_id=problem_id,
            algorithm=algorithm.lower(),
            status="staging",
            train_mode=train_mode,
            evaluation_strategy=evaluation_strategy,
            metrics_json=metrics,
            uri=None,
            metadata_json=metadata,
            explanation_json=explaination_summary,
            created_by=NAME,
            name=name,
        )
        metadata["model_id"] = model_id
        metadata["model_uri"] = model_uri
    else:
        metadata["model_id"] = model_id
        metadata["model_uri"] = model_uri
        update_model(
            model_id=model_id,
            status="staging",
            metrics_json=json.dumps(metrics),
            uri=model_uri,
            metadata_json=json.dumps(metadata),
            explanation_json=json.dumps(explaination_summary),
        )

    parent_path = Path(model_uri).parent

    if save_model(model, parent_path):
        logger.info(f"[SAVE_MODEL] Model saved at: {model_uri}")
        if save_metadata(metadata, parent_path):
            logger.info(f"[SAVE_MODEL_METADATA] Model's metadata saved at: {model_uri}")
            return model_id, model_uri
        else:
            logger.error(f"[SAVE_MODEL_METADATA] Failed to save model's metadata at: {model_uri}")
            raise RuntimeError(
                f"Failed to save the model's metadata at {parent_path}")
    else:
        logger.error(f"[SAVE_MODEL] Failed to save model at: {model_uri}")
        raise RuntimeError(f"Failed to save the model at {parent_path}")
